---
title: 'Short: RAFT'
author: 'Alexander Junge'
date: '2024-03-20'
slug: RAFT
categories:
  - short
tags:
  - ai
  - rag
draft: false
---

# RAFT: Adapting Language Model to Domain Specific RAG

From the [Gorilla LLM project](https://gorilla.cs.berkeley.edu), Retrieval Aware Fine-Tuning (RAFT) combines
retrieval-augmented generation and fine-tuning to adapt language models to domain-specific knowledge.

Blog post: [here](https://gorilla.cs.berkeley.edu/blogs/9_raft.html)

Paper: [here](https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/RAFT.pdf)

## Why

Retrieval Augmented Generation (RAG) and fine-tuning are two of the most important concepts in the NLP domain when it comes to
exposing large language models to recent, domain-specific information.

The Retrieval Aware Fine-Tuning (RAFT) model is a combination of both of these concepts and
generalizes [Retriever Aware Training (RAT)](https://gorilla.cs.berkeley.edu/blogs/3_retriever_aware_training.html).

## What

Image from the blog post [here](https://gorilla.cs.berkeley.edu/blogs/9_raft.html):

![](/posts/2024-03-20/raft.png)

- supervised finetuning (SFT) on positive and negative context documents
- chain-of-though finetuning, quoting segments from the context
- force model to memorize domain knowledge and disregard irrelevant documents by sometimes removing positive documents from SFT samples

## So what?

- RAFT achieves better performance on domain-specific open book tasks than RAG or SFT alone when using a small openly available model (Llama2-7B)
- But: no comparison against GPT4-class models (my intuition is that GPT4-class models outperform RAFT, especially in a RAG scenario))